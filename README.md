# BAIDU_AIAdversialAttack
AI安全对抗赛第二名方案               --本项目基于飞桨PaddlePaddle实现
队伍:[我不和你们玩了](https://github.com/sleepingxin)
## [赛题背景](https://aistudio.baidu.com/aistudio/competition/detail/15)
目前人工智能和机器学习技术被广泛应用在人机交互、推荐系统、安全防护等各个领域，其受攻击的可能性以及是否具备强抗打击能力备受业界关注。具体场景包括语音，图像识别，信用评估，防止欺诈，过滤恶意邮件，抵抗恶意代码攻击，网络攻击等等。攻击者也试图通过各种手段绕过，或直接对AI模型进行攻击达到对抗目的。在人机交互这一环节，随着移动设备的普及，语音、图像作为新兴的人机输入手段，其便捷和实用性被大众所欢迎。因此图像识别的准确性对人工智能产业至关重要。这一环节也是最容易被攻击者利用，通过对数据源的细微修改，在用户感知不到的情况下，使机器做出了错误的操作。这种方法会导致AI系统被入侵、错误命令被执行，执行后的连锁反应会造成的严重后果。

本次竞赛的题目和数据由百度安全部、百度大数据研究院提供，竞赛平台AI Studio由百度AI技术生态部提供。期待参赛者们能够以此为契机，学习对抗样本理论知识并提升深度学习工程实践能力。欢迎全球范围开发者积极参与，鼓励高校教师积极参与指导。
## 赛题描述
* 初赛：初赛中，选手通过对 指定图像 添加扰动，使目标模型（Target Model）（一个为ResNeXt50模型并公开模型结构与参数（白盒）；一个为MobileNetV2模型并公开模型结构与参数（白盒）；一个不公开模型结构与参数（黑盒）。）分类错误，例如对于一张分类为A的图片，目标模型只要判别扰动后的样本不为A，即可判定成功。同时以生成扰动量越小越优。
* 复赛：复赛选手的目标与初赛相同： 利用给定，将指定的120张图片样本生成为攻击样本，主办方根据选手提供的攻击样本在后台使用上述5个Target Model（一个是与初赛相同的ResNeXt50白盒模型（白盒），一个是人工加固的模型（灰盒），另外三个均为黑盒模型，其中包括由AutoDL技术训练的模型。）进行评估，只要使Target Model分类结果与Label不一致，则判定为攻击成功。样本攻击成功数越多、扰动越小，得分越高。

## 方案
攻击所用算法主体为MomentumIteratorAttack，代码实现参照了[advbox](https://github.com/advboxes/AdvBox)。在此基础之上，尝试了不同的目标函数，集成了不同的模型，添加了多样的越过局部最优的策略。
### 模型集成
集成模型选取思路为多元，尽可能多的不同模型，才可能逼近赛题背后的黑盒模型。
<div align=center><img src="https://github.com/sleepingxin/BAIDU_AIAdversialAttack/blob/master/pictures/ensembelmodel.png"/></div>
